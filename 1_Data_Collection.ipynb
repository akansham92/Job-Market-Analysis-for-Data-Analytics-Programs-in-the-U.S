{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPwP/Ir7FGJDCkPn8Gj9NBG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data collection with the help of SerpAPI\n","\n","Real time job details have been collected for various Data Science positions accross the United States."],"metadata":{"id":"pD06QAEQGku0"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"brsKoNpp0v9O","executionInfo":{"status":"ok","timestamp":1731705093757,"user_tz":480,"elapsed":4792,"user":{"displayName":"Padhma ram","userId":"04412407975349792346"}},"outputId":"7bf175f6-8b8c-4507-e9f3-bb0fca3bd5a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting serpapi\n","  Downloading serpapi-0.1.5-py2.py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from serpapi) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->serpapi) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->serpapi) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->serpapi) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->serpapi) (2024.8.30)\n","Downloading serpapi-0.1.5-py2.py3-none-any.whl (10 kB)\n","Installing collected packages: serpapi\n","Successfully installed serpapi-0.1.5\n"]}],"source":["!pip install serpapi\n","pip install google-search-results"]},{"cell_type":"code","source":["from serpapi import GoogleSearch\n","#from dataExtraction import extract\n","from IPython.display import clear_output\n","from pprint import pprint"],"metadata":{"id":"OigxXOcZ06vE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bIo-jKrWKyvf","executionInfo":{"status":"ok","timestamp":1731713557006,"user_tz":480,"elapsed":16957,"user":{"displayName":"Padhma ram","userId":"04412407975349792346"}},"outputId":"f452acc5-1ed8-45c6-9d4d-69455d1a3e36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from serpapi import GoogleSearch\n","import csv\n","from pprint import pprint  # For debugging\n","import requests  # We will use this to fetch the next page directly\n","\n","# API key and country setup\n","api_key = \"cc95ab69c225070c373d9c729ed84c365acfe7b37d6e8207b765a95462fc12f4\"\n","country = \"us\"  # You can switch this to another valid country code\n","\n","# Target number of job listings to collect (11,000 rows)\n","target_rows = 11000\n","\n","# Function to extract data from results and write to CSV\n","def extract_to_csv(results, csv_writer, total_rows_collected):\n","    if 'jobs_results' in results and results['jobs_results']:\n","        for job in results['jobs_results']:\n","            # Stop when the target number of rows is reached\n","            if total_rows_collected >= target_rows:\n","                return total_rows_collected\n","\n","            # Extract relevant job information\n","            title = job.get('title', 'N/A')\n","            company_name = job.get('company_name', 'N/A')\n","            location = job.get('location', 'N/A')\n","            via = job.get('via', 'N/A')\n","            description = job.get('description', 'N/A')\n","            job_highlights = ', '.join([item for highlight in job.get('job_highlights', []) for item in highlight.get('items', [])])\n","            detected_extensions = job.get('detected_extensions', {})\n","            date_posted = detected_extensions.get('posted_at', 'N/A')\n","            schedule_type = detected_extensions.get('schedule_type', 'N/A')\n","            job_id = job.get('job_id', 'N/A')\n","            share_link = job.get('share_link', 'N/A')\n","\n","            # Write the job information as a row to the CSV file\n","            csv_writer.writerow([title, company_name, location, via, description, job_highlights, date_posted, schedule_type, job_id, share_link])\n","\n","            total_rows_collected += 1  # Keep track of rows collected\n","    else:\n","        print(\"No job listings found in the API response.\")\n","\n","    return total_rows_collected\n","\n","# Open a CSV file to write the data\n","with open('job_results_Lead Data Scientist.csv', mode='w', newline='', encoding='utf-8') as file:\n","    csv_writer = csv.writer(file)\n","    # Write the header row\n","    csv_writer.writerow(['Title', 'Company Name', 'Location', 'Via', 'Description', 'Job Highlights', 'Date Posted', 'Schedule Type', 'Job ID', 'Share Link'])\n","\n","    # Initial search parameters\n","    params = {\n","        \"engine\": \"google_jobs\",\n","        \"q\": \"Lead Data Scientist\",  # Search query\n","        \"hl\": \"en\",\n","        \"api_key\": api_key,\n","        \"gl\": country\n","    }\n","\n","    search = GoogleSearch(params)\n","    results = search.get_dict()\n","\n","    total_rows_collected = 0  # Keep track of how many rows have been collected\n","    pages_collected = 0  # Keep track of how many pages have been collected\n","\n","    # Loop through pages using `next_page_token` and stop when target rows are collected\n","    while total_rows_collected < target_rows and 'jobs_results' in results:\n","        # Extract the current page's data\n","        total_rows_collected = extract_to_csv(results, csv_writer, total_rows_collected)\n","\n","        # Debug: Print current rows collected\n","        print(f\"Total rows collected so far: {total_rows_collected}\")\n","\n","        # Check if there's a next page token for pagination\n","        if 'serpapi_pagination' in results and 'next' in results['serpapi_pagination']:\n","            next_page_url = results['serpapi_pagination']['next']\n","\n","            # Debug: Print the next page URL\n","            print(f\"Next page URL: {next_page_url}\")\n","\n","            # Append the API key to the next page URL\n","            next_page_url_with_key = f\"{next_page_url}&api_key={api_key}\"\n","\n","            # Fetch the next page using the full URL with the API key\n","            response = requests.get(next_page_url_with_key)\n","            results = response.json()  # Convert response to dictionary format\n","            pages_collected += 1  # Increment the page count\n","        else:\n","            print(\"No next page token found, stopping pagination.\")\n","            break  # Exit loop if there are no more pages\n","\n","# Final output\n","print(f\"{pages_collected} pages of job listings collected.\")\n","print(f\"{total_rows_collected} job listings have been successfully written to 'job_results.csv'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CsqkImRYunxJ","executionInfo":{"status":"ok","timestamp":1731713150769,"user_tz":480,"elapsed":6749,"user":{"displayName":"Padhma ram","userId":"04412407975349792346"}},"outputId":"3fa680a1-e0c0-4300-e0f1-8c61550b8d26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total rows collected so far: 10\n","Next page URL: https://serpapi.com/search.json?engine=google_jobs&gl=us&google_domain=google.com&hl=en&next_page_token=eyJmYyI6IkVxSURDdUlDUVVwSE9VcHJUbmhCV0RWR1kwNUdkMHR5WmxWU1preEpjRVJLTFhkUGVURlNjVzkyY2pOMmVrWTFTR2xpVG5WRVpYRXhRMlpaYld0V1FXNXJiVWMzV1ZwVVpVaFlYelp1VTNjMU5qaFFVWHBqU210TWNUVnhNVkJ0Y25wRmJscFBiRU4xTVhseE1IUldkMGg1YVdaR01FcHFjelJyZG5wUWNERnZXQzFTVUZCa00wOUlRV0kyVlZKTVYyOVlVbkJaTW5GT1RtSkdiRmxKWlU1SVdYVnFhbWc1VGpadVdtcEViR1pNVDFoQlRVOHhOemxmT1Vab1RERjRVM05TWTNsSmNGbEZTR0ZXTUhad1QzSXhVMlI2TkUxMVJVZFJhbGRtU1c5U1JFVk1RWEJhYms0NWRGOVlWazFCV2s0MmFGZEplbVJxYTBkR1pqZEpRWFJCU25GVllUbE5UVEoxUWkxNWJUWnFiVzF2YzFsUFNYVlZhSFpJVm5wWmFtVmhURkl0T0Y5U2MweHpWVjlrVFRGVFNYRm5jVTFsWlc0NVpYcEJkR3d4VFZwVWVWSkNaVloxTlVSNFRWaElMVU5WZVdGNVdra3paa0l6YmpKcWJFNWZReTFrV1hOM0VoZGxaR2N6V2pkbFVFSlpTMGhvWWtsUWRtWXpXVFJSZHhvaVFVWllja1ZqY0UwME9HWmFjMngwZG5kQ1NYTm1PR0o0UVU1cFRYcHNlVGRMUVEiLCJmY3YiOiIzIn0%3D&q=Lead+Data+Scientist\n","Total rows collected so far: 20\n","Next page URL: https://serpapi.com/search.json?engine=google_jobs&gl=us&google_domain=google.com&hl=en&next_page_token=eyJmYyI6IkVxSURDdUlDUVVwSE9VcHJUVVkzV1hCMlJFRm9UalJ6U0RCRmNYaFVaMlYxVFdvd1JqTkRibTFDUm5GcVVGVkdkV1pJVTNKMmEzbHlNWEJITmtOQmQwNTFNblJaTW5KWlp6SmxiM2x6UTBsc1gyNWxTemhJVjNKRVRHZEVTblYwZUdRNGRXUllPRkJwZVZSMVIyb3hNek10YzJ0T2NsazRTbmM0UzFnMGVYVlpXRkJDVkV4dWF6bEpZVmx6V2w5WlRDMU1lRlJzZGpNeWFrRnhhMEZXYmxoeU1WaDZkVVpoYzFNeFRuSnlhbWhmZEdKUFRuWjFOeTB6WW1OdVUxbG1ia3RIYVhkVVVHaHFhRzlZY0VSMGMzVkhlVVJuVWxWaFdGTXdSazV3V0VGWWNEUTFabUUwVlhWNlVVTk9jbEpJYzNkaFJGUkViamRqUjJ4T01tdFhOR2gxU0d0aFRFeHZNM1pmYm5GSFZreHBkMGgyYjI5T1RtZGxRV2hxVkRKM09VTkRjVmRNUms1c1ozQnNkbWxRU20weFdHaFNVMWRITlRoaWRqWlNiVzlYUlMxV1NtNTNaVGxsUlVKTlpXMUlNazVPY0ZOQmFESkNNVVExYzFOemJVMDJUaTFrVFdwbkVoZGxPV2N6V2paMlUwVlpYMncxVG05UWNVeElZekJCVlJvaVFVWllja1ZqY2tweFNXeHZSR1pPWDFVNFEySnNkWFpIVUdWdmFEZEpVVnBUWnciLCJmY3YiOiIzIn0%3D&q=Lead+Data+Scientist\n","Total rows collected so far: 30\n","Next page URL: https://serpapi.com/search.json?engine=google_jobs&gl=us&google_domain=google.com&hl=en&next_page_token=eyJmYyI6IkVxSURDdUlDUVVwSE9VcHJUbGhLZGtObVVsUmhaMVUxVTBsMFJVeFNRMHRoUXpkUlF6ZzJWWGhRTWt4QlVXRXpORzB0WkdWUlRtY3lkVkJSYVdWeFpERm1RMnQxUW0xSE1sSTNiVlkyUmtWUGMyOW5iMEkzYXpoRVlXaFVaMmxmVkVoWVlqTkZOVjgzU0ZoWExUUlBTVko2TlZGMVRVVkdUbk5RTVZGR2RtdGtia1pHTFROS2NHZHlWM0F4UzBKcFJWcGZNbEZJVkRWblJ6SklZVWxwYW1aMU9FUk5NVFIwVWxoTU5sTnBMV0Z2TkhWRU5ITktjakpUU21SelNXaGZTV3R5VURablR6ZHNaM2xOUW5sRk9XdzRNblpuZDI1MmNqQTFNbTlTTjBsTFpuWlNOR2g2ZDNJelJGZHFSRlp2TFVkR1RWWlZYMEZFYXpOWlNGVm9lbWRhU0ZnMGVIRk9UMEp2VDFSbU4xTlZTbTkwWDJ3eVlrNVpjMVpMU1RKbE9GSnRRbUUyYUhrdFJWbFpkVWQ1YUVWVVpqQkVWRE5EWmxsTllXVkdSbEE1WldseFFqRkRSaTE0VERSRlpqVmhiRmsxTVVWS1RUZDVNbHBGZVdWTlVESnBjRkF3UkMxbkVoZG1UbWN6V2pVdFpVWkpSM0UxVG05UWFUaERkMnRSWXhvaVFVWllja1ZqY1dkSmMwZGZjVzlrVGpkalVIZFBlRXhoVEVGM1ZVcFJjRUZrUVEiLCJmY3YiOiIzIn0%3D&q=Lead+Data+Scientist\n","Total rows collected so far: 40\n","Next page URL: https://serpapi.com/search.json?engine=google_jobs&gl=us&google_domain=google.com&hl=en&next_page_token=eyJmYyI6IkVxSURDdUlDUVVwSE9VcHJUWEJuVEhKb01FNUxORTF3V1hkcVpsQmphMUJOVmtsWE5rRldaMUZsVGpGdk5ubzBaVFozYzI1aVJsQnJhMGwyVlROdGNqRTJYM1I0TVhkelNubG5URnBZU21Oa2EwOTZlbTltWW5kUE9EYzJZbFZYWW5GcFIzRnhOSGQ1ZW1KRVUyNVNSVmhKWmtsNVNsOU5NbFpVYlZnNFVFVlZja05EU0VWMmFUQk1XVzUzVlRGMFoyUndiRk5ZVFhWbmJISnZhMkZyWVV0SGFDMUVWamczTkVjNGFXWjBUR0p4VGxkUmQzTnFZMVJDZGkxMlNrdFpRMTh6VDBodU0wOWtXRVJ6VW5aTE5UZ3llR0p1Y0MxSVNFcGlOMDVJT0VSUFYzQXlkM2RoYmxaSmNsOWliM1p1U3pJelEycE1lamRXYUZsRGNXRkZaMVU0TlZGUVYzTnVTRmR3ZFU5SlpGTkhkREJ4TkhkSGExZzJhRnAwTjJsMllVODFkMHRTTjB4NmIzaHRaMVV3ZDJVNVpVUjVkM3BLZFVwUk1URnlWbkY2UXpKWmVrOVlTRGczYkVsa1l6QkhTM2hITjJGUWFWWXdSRlV3V0VWcmEyZEJTMVZrUVdSQkVoZG1aR2N6V2psVE0wOUtjbXcxVG05UWFqVklXamhCYXhvaVFVWllja1ZqY25JMlNUY3dOWHB2Y0dKTmRYSkVPRUZPUTNCYVFqTTBRVXhKVVEiLCJmY3YiOiIzIn0%3D&q=Lead+Data+Scientist\n","4 pages of job listings collected.\n","40 job listings have been successfully written to 'job_results.csv'.\n"]}]}]}